{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "import librosa\n",
    "import numpy\n",
    "import pickle\n",
    "import soundfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras\n",
    "import scipy\n",
    "\n",
    "\n",
    "print(\"Librosa version = \",librosa.__version__)\n",
    "print(\"keras version = \",tensorflow.keras.__version__)\n",
    "print(\"tensorflow version = \",tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fold='/Data2/DCASE/DCASE2020t1_B/TAU-urban-acoustic-scenes-2020-3class-development/'\n",
    "train_filename = data_fold + 'evaluation_setup/fold1_train.csv'\n",
    "test_filename = data_fold + 'evaluation_setup/fold1_evaluate.csv'\n",
    "meta_filename = os.path.join(data_fold, 'meta.csv')\n",
    "meta_db = pandas.read_csv(meta_filename, '\\t')\n",
    "scene_labels = meta_db[\"scene_label\"].unique().tolist()   # 3개 indoor, outdoor, transportation\n",
    "identifiers = meta_db[\"identifier\"].unique().tolist() # 514개\n",
    "\n",
    "meta_db = meta_db.to_dict('records')\n",
    "for path in meta_db:\n",
    "    path['filename'] = os.path.join(data_fold, path['filename'])\n",
    "    \n",
    "train_db=pandas.read_csv(train_filename,'\\t')\n",
    "train_db = train_db.to_dict('records')\n",
    "train_files=[]\n",
    "for path in train_db:\n",
    "    split_filename=path['filename'].split('-')\n",
    "    path['filename'] = os.path.join(data_fold, path['filename'])\n",
    "    path['identifier']= '-'.join(split_filename[1:3])\n",
    "    train_files.append(path['filename'])\n",
    "    \n",
    "test_db=pandas.read_csv(test_filename,'\\t')\n",
    "test_db = test_db.to_dict('records')\n",
    "test_files=[]\n",
    "for path in test_db:\n",
    "    split_filename=path['filename'].split('-')\n",
    "    path['filename'] = os.path.join(data_fold, path['filename'])\n",
    "    path['identifier']= '-'.join(split_filename[1:3])\n",
    "    test_files.append(path['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=train_db\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "training_files = []\n",
    "validation_files = []\n",
    "\n",
    "\n",
    "for scene_id, scene_label in enumerate(scene_labels):\n",
    "    scene_meta = [file for file in db if file['scene_label']==scene_label]\n",
    "    data = {}\n",
    "\n",
    "    unique_identifiers = [file[\"identifier\"] for file in scene_meta]\n",
    "    unique_identifiers.sort()\n",
    "    for identifier in unique_identifiers:\n",
    "        path = identifier.split(\"-\")\n",
    "        new_value=[file[\"filename\"] for file in scene_meta if file[\"identifier\"]==identifier]\n",
    "        if path[0] not in data:\n",
    "            data[path[0]] = {}\n",
    "\n",
    "        data[path[0]][path[1]] = new_value\n",
    "        \n",
    "    current_scene_validation_amount = []\n",
    "    sets_candidates = []\n",
    "\n",
    "    identifier_first_level = list(data.keys())\n",
    "\n",
    "    for i in range(100):\n",
    "        current_validation_files = []\n",
    "        current_training_files = []\n",
    "\n",
    "        current_validation_identifiers2 = 0\n",
    "        for identifier1 in identifier_first_level:\n",
    "            current_ids = list(data[identifier1].keys())\n",
    "            random.shuffle(current_ids, random.random)\n",
    "\n",
    "            validation_split_index = int(numpy.ceil(0.3 * len(current_ids)))\n",
    "            current_validation = current_ids[0:validation_split_index]\n",
    "            current_training = current_ids[validation_split_index:]\n",
    "\n",
    "            for identifier2 in current_validation:\n",
    "                current_validation_files += data[identifier1][identifier2]\n",
    "\n",
    "            for identifier2 in current_training:\n",
    "                current_training_files += data[identifier1][identifier2]\n",
    "\n",
    "            current_validation_identifiers2 += len(current_validation)\n",
    "            \n",
    "        current_scene_validation_amount.append(\n",
    "            len(current_validation_files) / float(\n",
    "                len(current_validation_files) + len(current_training_files))\n",
    "        ) # 전체 v파일에 대해서 validation_files의 비율\n",
    "\n",
    "        sets_candidates.append({\n",
    "            'validation': current_validation_files,\n",
    "            'training': current_training_files,\n",
    "            'validation_identifiers1': len(identifier_first_level),\n",
    "            'validation_identifiers2': current_validation_identifiers2,\n",
    "        })\n",
    "\n",
    "    best_set_id = numpy.argmin(numpy.abs(numpy.array(current_scene_validation_amount) - 0.3))\n",
    "\n",
    "    validation_files += sets_candidates[best_set_id]['validation']\n",
    "    training_files += sets_candidates[best_set_id]['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train set: ', len(training_files))\n",
    "print('val set: ', len(validation_files))\n",
    "print('test set: ', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name1 = '3class_melspecdelta_6'\n",
    "\n",
    "\n",
    "item_list_train = []\n",
    "item_list_validation = []\n",
    "item_list_test = []\n",
    "\n",
    "for item in meta_db:\n",
    "    _, current_last_level_path = os.path.split(item[\"filename\"])\n",
    "    base_filename, _ = os.path.splitext(current_last_level_path)\n",
    "\n",
    "    feature_filename1 = os.path.join(data_fold+'features/'\n",
    "                                     +feature_name1+'/'+base_filename+'.npz')\n",
    "\n",
    "    item_ = {\n",
    "        'data': {\n",
    "            'filename':[feature_filename1]\n",
    "        },\n",
    "        'meta': {\n",
    "            'label': item[\"scene_label\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if item[\"filename\"] in test_files:\n",
    "        item_list_test.append(item_)\n",
    "    elif item[\"filename\"] in training_files:\n",
    "        item_list_train.append(item_)\n",
    "    elif item[\"filename\"] in validation_files:\n",
    "        item_list_validation.append(item_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "for item in item_list_train:\n",
    "    binary_matrix = numpy.zeros((len(scene_labels), 1))\n",
    "    pos = scene_labels.index(item[\"meta\"][\"label\"])\n",
    "    binary_matrix[pos,:] = 1\n",
    "    \n",
    "    audio = numpy.load(item[\"data\"][\"filename\"][0])\n",
    "    embedding = audio['embedding']\n",
    "    \n",
    "    X_train.append(embedding)\n",
    "    Y_train.append(binary_matrix.T)\n",
    "    \n",
    "\n",
    "X_train = numpy.array(X_train)\n",
    "Y_train = numpy.vstack(Y_train)\n",
    "\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "for item in item_list_validation:\n",
    "    binary_matrix = numpy.zeros((len(scene_labels), 1))\n",
    "    pos = scene_labels.index(item[\"meta\"][\"label\"])\n",
    "    binary_matrix[pos,:] = 1\n",
    "    \n",
    "    audio = numpy.load(item[\"data\"][\"filename\"][0])\n",
    "    embedding = audio['embedding']\n",
    "    \n",
    "\n",
    "    X_val.append(embedding)\n",
    "    Y_val.append(binary_matrix.T)\n",
    "    \n",
    "\n",
    "X_val = numpy.array(X_val)\n",
    "Y_val = numpy.vstack(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "for item in item_list_test:\n",
    "    binary_matrix = numpy.zeros((len(scene_labels), 1))\n",
    "    pos = scene_labels.index(item[\"meta\"][\"label\"])\n",
    "    binary_matrix[pos,:] = 1\n",
    "    \n",
    "    audio = numpy.load(item[\"data\"][\"filename\"][0])\n",
    "    embedding = audio['embedding']\n",
    "    \n",
    "    X_test.append(embedding)\n",
    "    Y_test.append(binary_matrix.T)\n",
    "    \n",
    "\n",
    "X_test = numpy.array(X_test)\n",
    "Y_test = numpy.vstack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('deltas-deltadelta train shape: ',X_train.shape)\n",
    "print('deltas-deltadelta validation shape: ', X_val.shape)\n",
    "print('deltas-deltadelta test shape: ',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_labels=[]\n",
    "for i in item_list_test:\n",
    "    if i['meta']['label']=='indoor':\n",
    "        dev_test_labels.append(0)\n",
    "    elif i['meta']['label']=='transportation':\n",
    "        dev_test_labels.append(1)\n",
    "    else:\n",
    "        dev_test_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_val_labels=[]\n",
    "for i in item_list_validation:\n",
    "    if i['meta']['label']=='indoor':\n",
    "        dev_val_labels.append(0)\n",
    "    elif i['meta']['label']=='transportation':\n",
    "        dev_val_labels.append(1)\n",
    "    else:\n",
    "        dev_val_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mixup import MixupGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Input,Add, Permute, Cropping2D, Activation, Maximum,Dropout, Flatten, Dense, Conv2D, MaxPooling2D, MaxPool2D,BatchNormalization, Convolution2D, ReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization as BN\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, SeparableConv2D\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras.layers import  concatenate\n",
    "\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) BIR-ResNet RF-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def relu6(x):\n",
    "    \"\"\"Relu 6\n",
    "    \"\"\"\n",
    "    return K.relu(x, max_value=6.0)\n",
    "\n",
    "def _conv_block(inputs, filters, kernel, strides):\n",
    "    \"\"\"Convolution Block\n",
    "    This function defines a 2D convolution operation with BN and relu6.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation(relu6)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def _bottleneck(inputs, filters, kernel, t, alpha, s, r=False):\n",
    "    \"\"\"Bottleneck\n",
    "    This function defines a basic bottleneck structure.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        alpha: Integer, width multiplier.\n",
    "        r: Boolean, Whether to use the residuals.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    # Depth\n",
    "    tchannel = K.int_shape(inputs)[channel_axis] * t\n",
    "    # Width\n",
    "    cchannel = int(filters * alpha)\n",
    "\n",
    "    x = _conv_block(inputs, tchannel, (1, 1), (1, 1))\n",
    "\n",
    "    x = DepthwiseConv2D(kernel, strides=(s, s), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation(relu6)(x)\n",
    "\n",
    "    x = Conv2D(cchannel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "\n",
    "    if r:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def _inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n",
    "    \"\"\"Inverted Residual Block\n",
    "    This function defines a sequence of 1 or more identical layers.\n",
    "    # Arguments\n",
    "        inputs: Tensor, input tensor of conv layer.\n",
    "        filters: Integer, the dimensionality of the output space.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "        t: Integer, expansion factor.\n",
    "            t is always applied to the input size.\n",
    "        alpha: Integer, width multiplier.\n",
    "        s: An integer or tuple/list of 2 integers,specifying the strides\n",
    "            of the convolution along the width and height.Can be a single\n",
    "            integer to specify the same value for all spatial dimensions.\n",
    "        n: Integer, layer repeat times.\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    #x = _bottleneck(inputs, filters, kernel, t, alpha, strides)\n",
    "    inputs = Conv2D(filters, (1,1))(inputs)\n",
    "    for i in range(1, n):\n",
    "        x = _bottleneck(inputs, filters, kernel, t, alpha, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet_1_mb2(input_shape, alpha=1.0):\n",
    "    \n",
    "    input_tensor = input_shape\n",
    "    \n",
    "    def conv1_layer(x):    \n",
    "        x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "        x = Conv2D(64, (3, 3), strides=(2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = ZeroPadding2D(padding=(1,1))(x)\n",
    "        return x   \n",
    "    \n",
    "    def conv2_layer(x):         \n",
    "        x = MaxPooling2D((3, 3), 2)(x)     \n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        for i in range(3):\n",
    "            if (i == 0 or i ==1):\n",
    "                x = _inverted_residual_block(x, 32, (3, 3), t=2, alpha=alpha, strides=1, n=2)\n",
    "\n",
    "            else:\n",
    "                x = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Activation('relu')(x)\n",
    "\n",
    "                x = Conv2D(64, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Activation('relu')(x)\n",
    "\n",
    "                shortcut = x        \n",
    "\n",
    "        return x\n",
    "    \n",
    "    x = conv1_layer(input_tensor)\n",
    "    x = conv2_layer(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output_tensor = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb2_model = resnet_1_mb2(Input(X_train.shape[1:]))\n",
    "mb2_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "mb2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(mb2_model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model = resnet_1_mb2(Input(X_train.shape[1:]))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "    lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=100))\n",
    "\n",
    "    model_name = 'bir_rf1'+ str(i)\n",
    "    checkpoint=\"checkpoints-10times/\"+model_name+'_'+\"cp.h5\"\n",
    "\n",
    "    mc = ModelCheckpoint(checkpoint, monitor='val_categorical_accuracy',\n",
    "                          verbose=True, save_best_only=True,\n",
    "                          mode ='auto', period =1)\n",
    "\n",
    "    TrainDataGen = MixupGenerator(X_train, \n",
    "                              Y_train, \n",
    "                              batch_size=64,\n",
    "                              alpha=0.4)()\n",
    "\n",
    "    history1 = model.fit_generator(TrainDataGen,\n",
    "                                   validation_data = (X_val, Y_val),\n",
    "                               epochs=100,\n",
    "                               callbacks=[lr, mc],\n",
    "                               steps_per_epoch=np.ceil(len(X_train)/64)\n",
    "                              )\n",
    "\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_1_mb2(Input(X_train.shape[1:]))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model_name = 'bir_rf1'+ str(i)\n",
    "    checkpoint=\"checkpoints-10times/\"+model_name+'_'+\"cp.h5\"\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "    scores_test = model.predict(X_test, verbose=1)\n",
    "    y_pred_test = np.argmax(scores_test,axis=1)\n",
    "    y_real = np.argmax(Y_test,axis=1)\n",
    "    Overall_accuracy = np.sum(y_pred_test==dev_test_labels)/len(X_test)\n",
    "    print('Test accuracy:', Overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model_name = 'bir_rf1'+ str(i)\n",
    "    checkpoint=\"checkpoints-10times/\"+model_name+'_'+\"cp.h5\"\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "    scores_test = model.predict(X_val, verbose=1)\n",
    "    y_pred_test = np.argmax(scores_test,axis=1)\n",
    "    y_real = np.argmax(Y_val,axis=1)\n",
    "    Overall_accuracy = np.sum(y_pred_test==dev_val_labels)/len(X_val)\n",
    "    print('Test accuracy:', Overall_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
